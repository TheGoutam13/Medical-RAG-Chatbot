{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef1ba69-47b4-42e6-ba49-a797dc467376",
   "metadata": {},
   "source": [
    "# Medical Chatbot- G Version\n",
    "This Pipeline answers questions strictly from a provided PDF or text file. It loads the document, counts tokens for query + document_text, and compresses the text via LLM summarisation if it exceeds the model’s context limit. An input guardrail blocks unsafe or disallowed queries before any model call. A strict prompt instructs the LLM to answer only from the document or reply “Not found in document.” The output guardrail then checks numeric grounding, token overlap, and entity presence to prevent hallucinations. Modular functions, logging, and error handling make it production‑ready, safe, and maintainable for enterprise‑level document‑based QA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbfe57-a38c-4d94-9a2e-47af5b15be2e",
   "metadata": {},
   "source": [
    "![Workflow](workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd4ec8-595b-4503-a9a9-e30772f7de48",
   "metadata": {},
   "source": [
    "## Importing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00854a44-733f-4c46-9f80-c4c4ef452dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import Tuple, Optional, List\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a512dd7-8856-467d-abe9-ab8e7c50a500",
   "metadata": {},
   "source": [
    "## Logging Configuration\n",
    "\n",
    "A logger is used to record messages about what your program is doing while it runs — like a running diary for your code.\n",
    "\n",
    "In Our case, it’s configured to Capture events (info, warnings, errors) from the “Medical Chatbot G‑Version” pipeline, Format them with a timestamp, severity level, and message so they’re easy to read.Output them to the console in real time via StreamHandler.Avoid duplicates by checking if handlers already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ca1aa7-702c-4d30-a13b-a5b14fd842e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"Medical Chatbot G-Version\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler(stream=sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s] %(asctime)s - %(name)s - %(message)s\")\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7239c2-fd69-402c-ba2e-b7ce9059c455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac1aec2-812d-4acd-97ce-2590f04dc99c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54d4c44-b90e-4d8c-a0b0-49ccbd1f7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adjust the model and context window here. gpt-4o-mini supports large contexts (up to ~128k).\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "MODEL_CONTEXT_TOKENS = int(os.environ.get(\"MODEL_CONTEXT_TOKENS\", \"128000\"))  # Conservative default\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", None)\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\n",
    "        \"OPENAI_API_KEY is not set. Please set it in your environment to use the LLM.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4cb1aca-6aaa-442b-8f01-18ad8a1ff73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client once\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b52a97-8379-4d5c-8381-bde2aaf8decf",
   "metadata": {},
   "source": [
    "## Tokenization Utilities\n",
    "This sets up token counting for your LLM pipeline.\n",
    "\n",
    "`get_encoder(model_name)` tries to load a tiktoken encoder optimised for the given model (e.g., gpt-4). If the model isn’t recognised, it logs a warning and falls back to the generic \"cl100k_base\" encoding.\n",
    "\n",
    "ENCODER stores the chosen encoder for reuse.\n",
    "\n",
    "`count_tokens(text)` then uses this encoder to convert the input string into tokens and returns the token count. This is essential for checking whether your document + query fit within the model’s context window before sending them to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7821bf39-ad8b-4250-9e7b-830f5e008db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(model_name: str):\n",
    "    \"\"\"\n",
    "    Get a tiktoken encoder for the provided model.\n",
    "    Falls back to 'cl100k_base' if the specific model mapping is unknown.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return tiktoken.encoding_for_model(model_name)\n",
    "    except KeyError:\n",
    "        logger.warning(\"Unknown model for tiktoken; falling back to cl100k_base.\")\n",
    "        return tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "ENCODER = get_encoder(OPENAI_MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count tokens in a string using tiktoken.\"\"\"\n",
    "    return len(ENCODER.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87a9cb-f190-43e5-b835-ab403c104d1b",
   "metadata": {},
   "source": [
    "## Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfdce468-14a4-418e-8575-8482ed441bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load a document from a .pdf or .txt file and return its full text content.\n",
    "    - For PDFs, it extracts text page-by-page.\n",
    "    - For text files, it reads directly.\n",
    "    Raises:\n",
    "        FileNotFoundError, ValueError, Exception\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.error(f\"File not found: {file_path}\")\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            logger.info(f\"Loading PDF: {file_path}\")\n",
    "            reader = PdfReader(file_path)\n",
    "            pages_text = []\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to extract text from page {i}: {e}\")\n",
    "                    page_text = \"\"\n",
    "                pages_text.append(page_text)\n",
    "            document_text = \"\\n\".join(pages_text).strip()\n",
    "        elif ext in [\".txt\", \".md\"]:\n",
    "            logger.info(f\"Loading text file: {file_path}\")\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                document_text = f.read().strip()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only .pdf and .txt/.md are supported.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Unexpected error while loading document.\")\n",
    "        raise e\n",
    "\n",
    "    if not document_text:\n",
    "        raise ValueError(\"Document appears to be empty after extraction.\")\n",
    "    return document_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabe0ce-0a7c-4117-9f7d-6a6e5b09720b",
   "metadata": {},
   "source": [
    "## Token Budget Check\n",
    "\n",
    "This function checks if your query plus document text will fit inside the model’s context window.\n",
    "\n",
    "It works by Counting tokens in the query and document using count_tokens().Adding them to get total_tokens.Comparing total_tokens to budget_tokens (the model’s max context size) to set fits as True or False.Logging the result with details for debugging and monitoring.\n",
    "\n",
    "Returning a tuple:\n",
    "\n",
    "fits → whether it’s within budget\n",
    "\n",
    "total_tokens → the actual combined token count\n",
    "\n",
    "This ensures you don’t send more text than the model can handle, preventing truncation or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbdefe5-0fd4-4f68-9751-f7c21102b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_budget(query: str, document_text: str, budget_tokens: int) -> Tuple[bool, int]:\n",
    "    \"\"\"\n",
    "    Return whether query + document_text fits within the model's context window,\n",
    "    and the total tokens used for that pair.\n",
    "    \"\"\"\n",
    "    total_tokens = count_tokens(query) + count_tokens(document_text)\n",
    "    fits = total_tokens <= budget_tokens\n",
    "    logger.info(f\"Token check: total={total_tokens}, budget={budget_tokens}, fits={fits}\")\n",
    "    return fits, total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878f8d4-254f-46cf-b17d-f261ce22b246",
   "metadata": {},
   "source": [
    "## Input Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e6071f-867c-4366-ae64-25dbc432dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_input_guardrail(query: str, document_text: str) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Validate the input for safety and basic relevance. Returns:\n",
    "      - allowed: bool\n",
    "      - message: None if allowed, else safe explanatory message\n",
    "\n",
    "    Rules:\n",
    "      - Block explicit requests for harmful instructions (e.g., how to create biological agents, weapons).\n",
    "      - Block requests unrelated to the document if they also ask for dangerous/illegal guidance.\n",
    "      - Allow neutral/medical queries relevant to the document.\n",
    "    \"\"\"\n",
    "    q_lower = query.lower()\n",
    "\n",
    "    # Disallowed content patterns (expand as needed)\n",
    "    disallowed_patterns = [\n",
    "        r\"make\\s+(a|an)\\s+(bomb|weapon|explosive)\",\n",
    "        r\"how\\s+to\\s+(manufacture|create)\\s+(biological|chemical)\\s+(weapon|agent)\",\n",
    "        r\"bypass\\s+(security|authentication)\",\n",
    "        r\"exploit\\s+(a|the)\\s+vulnerability\",\n",
    "        r\"harm\\s+(someone|people)\",\n",
    "        r\"kill\\s+(someone|people)\",\n",
    "    ]\n",
    "    for pat in disallowed_patterns:\n",
    "        if re.search(pat, q_lower):\n",
    "            logger.warning(\"Input guardrail: disallowed/harmful request detected.\")\n",
    "            return False, \"Your request cannot be processed because it seeks unsafe or disallowed information.\"\n",
    "\n",
    "    # Basic irrelevance detection (soft): If the query appears entirely unrelated to the document's domain,\n",
    "    # we still allow but the pipeline will likely yield \"Not found in document.\" We only block if it's both\n",
    "    # irrelevant and seeking risky info (already handled). So we proceed.\n",
    "\n",
    "    # Medical safety: Disclaimers are handled by answering strictly from the document.\n",
    "    return True, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102706a-8377-4b63-af17-ed97b758cffe",
   "metadata": {},
   "source": [
    "## Helper: Chunking\n",
    "This function splits long text into token‑bounded chunks so they fit within an LLM’s context window.\n",
    "\n",
    "It first encodes the text into tokens using the global ENCODER. It calculates an overlap — the smaller of the given overlap or 10% of max_tokens_per_chunk — to preserve context between chunks. In a loop, it slices the token list from start to end (capped at max_len), decodes that slice back into text, and appends it to chunks. If the end of the token list is reached, it stops; otherwise, it moves start forward but steps back by overlap tokens to create the overlap.\n",
    "\n",
    "This ensures each chunk is small enough for the model but still retains continuity for summarisation or sequential processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b58f05a2-26ab-4171-b118-633b6f8e18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens_per_chunk: int, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk text into token-bounded segments with overlap to preserve context for summarization.\n",
    "    \"\"\"\n",
    "    tokens = ENCODER.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    max_len = max_tokens_per_chunk\n",
    "    ov = min(overlap, max(0, max_len // 10))\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_len, len(tokens))\n",
    "        chunk = ENCODER.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - ov  # overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d3d13-c3a2-474d-81c1-cc905d094f62",
   "metadata": {},
   "source": [
    "## Compress Document via Summarization \n",
    "This function uses an LLM to produce a concise, fact‑based summary of a given text, with a word limit (max_words, default 250)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b286b4-66ef-4a15-a653-50ca9eddb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _llm_summarize(text: str, max_words: int = 250) -> str:\n",
    "    \"\"\"\n",
    "    Summarize a text using the LLM into a concise, factual summary.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a precise summarizer. Create a concise, faithful summary capturing key facts, \"\n",
    "        f\"definitions, indications, contraindications, doses, adverse effects, and monitoring steps in <= {max_words} words. \"\n",
    "        \"Do not invent information. Only use the provided text.\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\nSummary:\"\n",
    "    )\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful, faithful medical summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.exception(\"LLM summarization failed.\")\n",
    "        # Fallback: simple truncation if LLM fails\n",
    "        words = re.split(r\"\\s+\", text)\n",
    "        return \" \".join(words[:max_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc512083-8750-49c4-b95e-1d7861da59da",
   "metadata": {},
   "source": [
    "This `compress_document` function ensures that a document plus the user’s query will fit within the model’s context limit by progressively compressing the text.\n",
    "\n",
    "It first reserves 1000 tokens for the prompt and query, then calculates a safe chunk_tokens_limit for splitting the document. Using chunk_text(), it breaks the document into overlapping segments small enough for the summariser model.Each chunk is summarised with _llm_summarize() into ≤250 words, and the summaries are concatenated. It then checks the combined summary against the token budget with check_token_budget().\n",
    "\n",
    "If still too large, it iteratively re‑summarises the combined text (≤200 words) up to three times, logging each step, until it fits. This guarantees the final compressed document is concise, faithful, and within the model’s context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d8e8df7-0a5a-491f-9cab-56098042816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_document(document_text: str, query: str, budget_tokens: int) -> str:\n",
    "    \"\"\"\n",
    "    Compress the document so that query + compressed_document fits within the budget.\n",
    "    Strategy:\n",
    "      - Chunk the document to safe sizes.\n",
    "      - Summarize each chunk.\n",
    "      - Iteratively reduce until within budget.\n",
    "    \"\"\"\n",
    "    # Reserve a portion of the context for prompt and query\n",
    "    reserved_for_prompt = 1000\n",
    "    per_chunk_limit = 4000  # tokens per input to the summarizer model\n",
    "    chunk_tokens_limit = min(per_chunk_limit, max(1000, (budget_tokens - reserved_for_prompt) // 4))\n",
    "\n",
    "    chunks = chunk_text(document_text, chunk_tokens_limit)\n",
    "    logger.info(f\"Compress: initial chunks={len(chunks)}, tokens per chunk≈{chunk_tokens_limit}\")\n",
    "\n",
    "    # Summarize each chunk\n",
    "    summaries = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        logger.info(f\"Summarizing chunk {i+1}/{len(chunks)}\")\n",
    "        summaries.append(_llm_summarize(ch, max_words=250))\n",
    "\n",
    "    combined = \"\\n\".join(summaries)\n",
    "\n",
    "    # If still too large, iteratively summarize again\n",
    "    fits, total = check_token_budget(query, combined, budget_tokens)\n",
    "    iteration = 0\n",
    "    while not fits and iteration < 3:\n",
    "        iteration += 1\n",
    "        logger.info(f\"Re-summarizing (iteration {iteration}) because tokens={total} exceed budget={budget_tokens}\")\n",
    "        combined = _llm_summarize(combined, max_words=200)\n",
    "        fits, total = check_token_budget(query, combined, budget_tokens)\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580b545-767e-4e2b-a444-dad977106689",
   "metadata": {},
   "source": [
    "## Prompt Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec75a718-de25-4ced-a50d-c83501a4e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query: str, document_text: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Build strict prompt instructing the model to use only the provided document text.\n",
    "    If answer not in document, it must reply exactly: 'Not found in document.'\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are an expert clinical QA assistant. Follow instructions exactly.\"\n",
    "    )\n",
    "    user = (\n",
    "        \"You must answer using ONLY the following document.\\n\"\n",
    "        \"If the answer is not in the document, reply exactly: 'Not found in document.'\\n\\n\"\n",
    "        \"Document:\\n\"\n",
    "        \"-----\\n\"\n",
    "        f\"{document_text}\\n\"\n",
    "        \"-----\\n\\n\"\n",
    "        f\"Question: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a33d9-ea4c-4b61-90d3-58da91774aa9",
   "metadata": {},
   "source": [
    "## LLM Query Execution\n",
    "`query_llm` sends messages to the LLM, retries on transient errors, logs failures, and returns the model’s response. It limits retries, delays between attempts, and raises exceptions after exceeding the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1209ab0-f844-4bb4-9ddc-03c8de28d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(messages: List[dict], temperature: float = 0.0, max_retries: int = 2, retry_delay: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Query the LLM with retries on transient errors.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=temperature,\n",
    "                messages=messages,\n",
    "            )\n",
    "            answer = resp.choices[0].message.content.strip()\n",
    "            return answer\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            logger.warning(f\"LLM query failed (attempt {attempt}): {e}\")\n",
    "            if attempt > max_retries:\n",
    "                logger.exception(\"Exceeded max retries for LLM call.\")\n",
    "                raise\n",
    "            time.sleep(retry_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa403e2b-bb58-40b9-8154-0a7276c601fc",
   "metadata": {},
   "source": [
    "## Output Guardrail\n",
    "\n",
    "This defines two helper functions for output guardrail checks.\n",
    "\n",
    "`_extract_numbers()` uses a regex to find all numeric tokens in a string, including integers and decimals. This supports factual validation by comparing numbers in the LLM’s answer against those in the source document.\n",
    "\n",
    "`_token_overlap_score()` measures Jaccard similarity between two texts. It tokenises each string into lowercase alphanumeric words, converts them to sets, and computes the ratio of the intersection size to the union size. If either set is empty, it returns 0.0. This score helps detect hallucinations by checking how much the answer’s vocabulary overlaps with the document’s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c31ce8-7325-44c6-87c5-d0b9c11a7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_numbers(text: str) -> List[str]:\n",
    "    \"\"\"Extract numeric tokens (including decimals) for simple factual cross-checks.\"\"\"\n",
    "    return re.findall(r\"\\b\\d+(?:\\.\\d+)?\\b\", text)\n",
    "\n",
    "def _token_overlap_score(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute a simple token overlap score between strings a (answer) and b (document).\n",
    "    Returns Jaccard similarity over lowercased word sets filtered to alphanumerics.\n",
    "    \"\"\"\n",
    "    tokenize = lambda s: set(re.findall(r\"[a-z0-9]+\", s.lower()))\n",
    "    A, B = tokenize(a), tokenize(b)\n",
    "    if not A or not B:\n",
    "        return 0.0\n",
    "    inter = len(A & B)\n",
    "    union = len(A | B)\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bc323-61e6-473a-b2e6-0e504538949a",
   "metadata": {},
   "source": [
    "`apply_output_guardrail` checks an LLM’s answer against the source document for numeric accuracy, token overlap, and entity presence, with optional relaxed summary mode, returning “Not found in document.” if grounding checks fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e5c164-d5dd-4b50-87ed-9fdde9181ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_output_guardrail(answer: str, document_text: str, allow_summary_mode: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Validate that the LLM response is grounded in document_text.\n",
    "\n",
    "    Parameters:\n",
    "        answer (str): The raw LLM answer.\n",
    "        document_text (str): The source document text.\n",
    "        allow_summary_mode (bool): If True, relaxes entity grounding checks\n",
    "                                   for summary-style queries while keeping\n",
    "                                   numeric and overlap checks.\n",
    "\n",
    "    Returns:\n",
    "        str: The validated answer, or 'Not found in document.' if it fails checks.\n",
    "    \"\"\"\n",
    "    canonical_nf = \"Not found in document.\"\n",
    "    if answer.strip() == canonical_nf:\n",
    "        return canonical_nf\n",
    "\n",
    "    # --- Summary mode: relaxed entity grounding ---\n",
    "    if allow_summary_mode:\n",
    "        # Numeric grounding check\n",
    "        ans_numbers = set(_extract_numbers(answer))\n",
    "        doc_numbers = set(_extract_numbers(document_text))\n",
    "        if ans_numbers and not ans_numbers.issubset(doc_numbers):\n",
    "            logger.warning(\"Summary mode: numeric values not grounded in document.\")\n",
    "            return canonical_nf\n",
    "\n",
    "        # Overlap heuristic\n",
    "        overlap = _token_overlap_score(answer, document_text)\n",
    "        if overlap < 0.02:\n",
    "            logger.warning(f\"Summary mode: low grounding overlap (score={overlap:.3f}).\")\n",
    "            return canonical_nf\n",
    "\n",
    "        # Skip strict entity grounding in summary mode\n",
    "        return answer\n",
    "\n",
    "    # --- Normal strict mode ---\n",
    "    # Numeric grounding\n",
    "    ans_numbers = set(_extract_numbers(answer))\n",
    "    doc_numbers = set(_extract_numbers(document_text))\n",
    "    if ans_numbers and not ans_numbers.issubset(doc_numbers):\n",
    "        logger.warning(\"Output guardrail: numeric values not grounded in document.\")\n",
    "        return canonical_nf\n",
    "\n",
    "    # Overlap heuristic\n",
    "    overlap = _token_overlap_score(answer, document_text)\n",
    "    if overlap < 0.05:\n",
    "        logger.warning(f\"Output guardrail: low grounding overlap (score={overlap:.3f}).\")\n",
    "        return canonical_nf\n",
    "\n",
    "    # Entity grounding\n",
    "    meds_in_answer = re.findall(r\"[A-Z][a-zA-Z0-9\\-]{3,}\", answer)\n",
    "    meds_in_answer = [m for m in meds_in_answer if m.lower() not in {\"not\", \"found\", \"document\"}]\n",
    "    for med in meds_in_answer:\n",
    "        if med.lower() not in document_text.lower():\n",
    "            logger.warning(f\"Output guardrail: entity '{med}' not grounded in document.\")\n",
    "            return canonical_nf\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319c2c1-fbf5-4483-bddf-8b217141ee6c",
   "metadata": {},
   "source": [
    "## Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955f695b-23f2-4f7c-b05b-ff7731ad6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_direct_document_qa(file_path: str, query: str, model_context_tokens: int = MODEL_CONTEXT_TOKENS) -> str:\n",
    "    \"\"\"\n",
    "    Orchestrates the full Direct-Document QA workflow.\n",
    "    Steps:\n",
    "      1) Load document\n",
    "      2) Token budget check; compress if needed\n",
    "      3) Input guardrail\n",
    "      4) Build prompt\n",
    "      5) Query LLM\n",
    "      6) Output guardrail\n",
    "    \"\"\"\n",
    "    # 1) Load\n",
    "    document_text = load_document(file_path)\n",
    "\n",
    "    # 2) Budget\n",
    "    fits, total_tokens = check_token_budget(query, document_text, model_context_tokens)\n",
    "    if not fits:\n",
    "        logger.info(\"Query + document exceeds context. Compressing document.\")\n",
    "        document_text = compress_document(document_text, query, model_context_tokens)\n",
    "        # Re-check after compression (defensive)\n",
    "        fits, _ = check_token_budget(query, document_text, model_context_tokens)\n",
    "        if not fits:\n",
    "            # Final defensive truncation if still too large (rare due to iterative summarize)\n",
    "            logger.warning(\"After compression, content still too large. Applying hard truncation.\")\n",
    "            # Keep last part to retain dosage and contraindications often toward the end\n",
    "            doc_tokens = ENCODER.encode(document_text)\n",
    "            keep = max(1000, model_context_tokens // 2)\n",
    "            document_text = ENCODER.decode(doc_tokens[-keep:])\n",
    "\n",
    "    # 3) Input guardrail\n",
    "    allowed, message = apply_input_guardrail(query, document_text)\n",
    "    if not allowed:\n",
    "        return message or \"Your request cannot be processed at this time.\"\n",
    "\n",
    "    # 4) Prompt\n",
    "    messages = build_prompt(query, document_text)\n",
    "\n",
    "    # 5) Execute\n",
    "    raw_answer = query_llm(messages, temperature=0.0)\n",
    "\n",
    "    # 6) Output guardrail\n",
    "    is_summary_query = \"summary\" in query.lower() or \"summarise\" in query.lower()\n",
    "    final_answer = apply_output_guardrail(raw_answer, document_text, allow_summary_mode=is_summary_query)\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdd06a-238b-4485-8107-a6c35cd7e6e9",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb02a1e-805c-4937-b52f-80d3bd820016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-12 15:26:12,076 - Medical Chatbot G-Version - Loading PDF: Drug Information Sheet.pdf\n",
      "[INFO] 2025-09-12 15:26:12,114 - Medical Chatbot G-Version - Token check: total=363, budget=128000, fits=True\n",
      "\n",
      "=== Answer ===\n",
      "The maintenance dose of remdesivir is 100 mg IV daily for 4 to 9 days depending on clinical response.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_file = \"Drug Information Sheet.pdf\"  # Replace with your file path (.pdf or .txt)\n",
    "    example_query = \"What is the maintenance dose of remdesivir?\"\n",
    "\n",
    "    try:\n",
    "        response = run_direct_document_qa(example_file, example_query, MODEL_CONTEXT_TOKENS)\n",
    "        print(\"\\n=== Answer ===\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Pipeline execution failed.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2436d-9240-4031-8cf8-4e7f42f7c559",
   "metadata": {},
   "source": [
    "### Test Query- Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1b9645f-d5b9-49e3-b76c-4e22cfbe8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-12 15:26:14,404 - Medical Chatbot G-Version - Loading PDF: Drug Information Sheet.pdf\n",
      "[INFO] 2025-09-12 15:26:14,521 - Medical Chatbot G-Version - Token check: total=365, budget=128000, fits=True\n",
      "\n",
      "=== Answer ===\n",
      "The document is a drug information sheet on remdesivir, an antiviral agent used for treating COVID-19. It details the drug's classification as a nucleoside analogue RNA polymerase inhibitor, which works by inhibiting viral RNA-dependent RNA polymerase to reduce SARS-CoV-2 replication. Remdesivir is indicated for hospitalized patients with confirmed COVID-19 who require supplemental oxygen but are not on invasive ventilation. The dosing regimen includes a loading dose of 200 mg IV on Day 1, followed by a maintenance dose of 100 mg IV daily for 4 to 9 days based on clinical response. Adverse effects may include nausea and transaminase elevations, with rare occurrences of hypersensitivity reactions and renal dysfunction. Monitoring of liver and renal function tests is required both at baseline and periodically during treatment. Contraindications include severe renal impairment (eGFR <30 mL/min) and known hypersensitivity to remdesivir. The document cites two references for further reading on the management and safety monitoring of remdesivir in hospitalized patients.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_file = \"Drug Information Sheet.pdf\"  # Replace with your file path (.pdf or .txt)\n",
    "    example_query = \"Summarise the entire document in under 200 words.\"\n",
    "\n",
    "    try:\n",
    "        response = run_direct_document_qa(example_file, example_query, MODEL_CONTEXT_TOKENS)\n",
    "        print(\"\\n=== Answer ===\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Pipeline execution failed.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46812b87-0a21-4ac1-9846-8ecad79f3e0d",
   "metadata": {},
   "source": [
    "### Test Query- Input Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e315e91-135d-4a02-b8e1-ba6d37c7eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-12 15:26:20,424 - Medical Chatbot G-Version - Loading PDF: Drug Information Sheet.pdf\n",
      "[INFO] 2025-09-12 15:26:20,538 - Medical Chatbot G-Version - Token check: total=359, budget=128000, fits=True\n",
      "[WARNING] 2025-09-12 15:26:20,540 - Medical Chatbot G-Version - Input guardrail: disallowed/harmful request detected.\n",
      "\n",
      "=== Answer ===\n",
      "Your request cannot be processed because it seeks unsafe or disallowed information.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_file = \"Drug Information Sheet.pdf\"  # Replace with your file path (.pdf or .txt)\n",
    "    example_query = \"Give me instructions to kill someone\"\n",
    "\n",
    "    try:\n",
    "        response = run_direct_document_qa(example_file, example_query, MODEL_CONTEXT_TOKENS)\n",
    "        print(\"\\n=== Answer ===\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Pipeline execution failed.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97bacb-a435-4df2-a341-f2957fc87dc6",
   "metadata": {},
   "source": [
    "### Test Query- Output Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef502be7-8231-45e2-a3a6-68a74f0247f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-09-12 15:26:20,565 - Medical Chatbot G-Version - Loading PDF: Drug Information Sheet.pdf\n",
      "[INFO] 2025-09-12 15:26:20,672 - Medical Chatbot G-Version - Token check: total=362, budget=128000, fits=True\n",
      "[WARNING] 2025-09-12 15:26:21,419 - Medical Chatbot G-Version - Output guardrail: low grounding overlap (score=0.015).\n",
      "\n",
      "=== Answer ===\n",
      "Not found in document.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    example_file = \"Drug Information Sheet.pdf\"  # Replace with your file path (.pdf or .txt)\n",
    "    example_query = \"What is the maximum treatment duration in days?\"\n",
    "\n",
    "    try:\n",
    "        response = run_direct_document_qa(example_file, example_query, MODEL_CONTEXT_TOKENS)\n",
    "        print(\"\\n=== Answer ===\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Pipeline execution failed.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9073ef2a-c6df-40a9-8cb9-f061a481cd3b",
   "metadata": {},
   "source": [
    "## Medical Chatbot- Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f23613fb-511b-418c-a47a-65717635dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def qa_interface(file_path, query):\n",
    "    if not file_path or not query.strip():\n",
    "        return \"Please upload a document and enter a query.\"\n",
    "\n",
    "    try:\n",
    "        is_summary_query = \"summary\" in query.lower() or \"summarise\" in query.lower()\n",
    "        answer = run_direct_document_qa(file_path, query, MODEL_CONTEXT_TOKENS)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error in Gradio interface call.\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks(title=\"Medical Chatbot-G Version\") as demo:\n",
    "    gr.Markdown(\"## Medical CHATBOT- G Version\\nUpload a PDF or TXT and ask a question. The system will only answer from the document.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\".pdf\", \".txt\"], type=\"filepath\")\n",
    "        query_input = gr.Textbox(label=\"Your Question\", placeholder=\"e.g., What is the recommended dosage?\")\n",
    "\n",
    "    answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
    "\n",
    "    submit_btn = gr.Button(\"RUN\")\n",
    "    submit_btn.click(fn=qa_interface, inputs=[file_input, query_input], outputs=answer_output)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831555f-2ee1-432c-ab74-9e9a559649fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
