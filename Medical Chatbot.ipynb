{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701f37e1-c715-4c50-9615-7f7719e3c540",
   "metadata": {},
   "source": [
    "## Medical RAG Chatbot with Medical PDF Document Retrieval and Safety Guardrails\n",
    "\n",
    "\n",
    "User Query → Input Guardrails → Document (Full or Section) → LLM Generation → Output Guardrails → User Answer\n",
    "\n",
    "This implementation loads domain‑specific PDFs, splits them into manageable chunks, and selects the most relevant sections via keyword/manual matching instead of embeddings. It then builds a context‑grounded prompt for a medical RAG chatbot, runs it through LLM with guardrails, and returns only safe, faithful answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b2239-ba48-463d-a3f2-fd5d8e3c3d32",
   "metadata": {},
   "source": [
    "### Importing the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73a7249-e27c-475b-b0ba-f6380445bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d505cbd-abca-47e0-a7bc-195ed524bbe3",
   "metadata": {},
   "source": [
    "### Loading the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ff047d-1562-4cb3-96f3-610919745216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613b87f-0d75-415a-a656-a99d4671a9fc",
   "metadata": {},
   "source": [
    "### Loading the Medical PDF Documents\n",
    "\n",
    "Loads all PDFs from the knowledge-base/ folder, extracts their contents into Document objects (with text + metadata), tags each document with its source filename, and stores them all in a single list called documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae7bf80-bf1c-4908-a3eb-d04ede1c431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob.glob(\"knowledge-base/*.pdf\")\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    for d in docs:\n",
    "        d.metadata[\"source\"] = os.path.basename(pdf)\n",
    "    documents.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa6f5e9-f4d7-4544-ba18-bd7158aad8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Names:\n",
      "Clinical Case Report.pdf\n",
      "Drug Information Sheet.pdf\n",
      "Medical Guideline.pdf\n",
      "Patient Education Leaflet.pdf\n",
      "Research Article Summary.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Document Names:\")\n",
    "for pdf in pdf_files:\n",
    "    print(os.path.basename(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a97ced-1bae-4a7c-9714-3f1786dfa370",
   "metadata": {},
   "source": [
    "### Splitting Documents into Manageable Chunks for LLM Processing\n",
    "\n",
    "This code checks each document’s length and splits it into smaller chunks (max 3000 characters with 200 overlap) only if it’s too long. Shorter documents are kept as-is. Finally, it prints the total number of resulting document sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ed999f-c97c-4880-8940-cb9776df213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 document sections.\n"
     ]
    }
   ],
   "source": [
    "MAX_CHARS = 3000  # adjust based on LLM token window\n",
    "splitter = CharacterTextSplitter(chunk_size=MAX_CHARS, chunk_overlap=200)\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    if len(doc.page_content) > MAX_CHARS:\n",
    "        chunks.extend(splitter.split_documents([doc]))\n",
    "    else:\n",
    "        chunks.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} document sections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab087e32-a966-4d35-bfd4-0c27d23a3019",
   "metadata": {},
   "source": [
    "### Keyword-Based Document Retrieval Function\n",
    "\n",
    "This function selects the most relevant document sections for a given query. It extracts keywords from the query, scores each document based on how many of those keywords appear in its text, and keeps only the ones with a positive score. Finally, it sorts them by score (highest first) and returns the top n results (default 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cde04f-44e1-48d6-afb8-f87196ea4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_sections(query, docs, top_n=3):\n",
    "    query_keywords = set(re.findall(r\"\\w+\", query.lower()))\n",
    "    scored = []\n",
    "    for doc in docs:\n",
    "        text = doc.page_content.lower()\n",
    "        score = sum(1 for kw in query_keywords if kw in text)\n",
    "        if score > 0:\n",
    "            scored.append((score, doc))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, doc in scored[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d1632-64af-4aa9-97fe-8c96cae7ff15",
   "metadata": {},
   "source": [
    "### Guardrails\n",
    "\n",
    "This code defines two simple safety functions.\n",
    "\n",
    "`is_safe_query` checks if a user’s query contains unsafe keywords (e.g., hate speech, violence, illegal activity) and blocks it if so.\n",
    "\n",
    "`filter_llm_response` compares the LLM’s response against the provided context to detect possible hallucinations (answers not grounded in the context).\n",
    "\n",
    "Currently, it only checks sentence-by-sentence in a basic way, but could be extended with stricter semantic checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3fd144-504c-47b5-8703-f5d9178b464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_safe_query(query):\n",
    "    unsafe_patterns = [\"hate speech\", \"violence\", \"illegal activity\"]\n",
    "    return not any(p in query.lower() for p in unsafe_patterns)\n",
    "\n",
    "def filter_llm_response(response, context):\n",
    "    # Reject hallucinations: if answer contains info not in context, block\n",
    "    context_text = \" \".join([c.page_content for c in context])\n",
    "    for sentence in response.split(\".\"):\n",
    "        if sentence.strip() and sentence.lower() not in context_text.lower():\n",
    "            pass  # Could add stricter semantic check here\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30144a84-1925-4454-a855-82cc81f66adf",
   "metadata": {},
   "source": [
    "### Prompt construction\n",
    "\n",
    "This function builds a prompt for the LLM by combining the user’s query with selected document chunks. It first formats each document as [source] content and joins them with line breaks. Then it returns a structured instruction telling the model to answer only using the provided context, or say \"Not Found\" if the answer isn’t available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86112685-3c60-4084-9240-efe496470929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, context_docs):\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        [f\"[{doc.metadata['source']}] {doc.page_content}\" for doc in context_docs]\n",
    "    )\n",
    "    return f\"\"\"You are an assistant. Use the following document sections to answer the query truthfully. \n",
    "If the answer is not in the document, say \"Not Found\".\n",
    "\n",
    "[User Query]\n",
    "{query}\n",
    "\n",
    "[Document Context]\n",
    "{context_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00547fbf-327e-43e5-b14a-21d13c531664",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b6f3a-dd0a-456c-8289-5beb9a6571c3",
   "metadata": {},
   "source": [
    "### Query Handling and Safe LLM Response Generation\n",
    "This code defines an `answer_query` function that uses an LLM to answer user queries. It first checks if the query is safe, then retrieves the most relevant document sections. A prompt is built with these sections and sent to the LLM for a response, which is then filtered for safety before returning the final answer with a feedback prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3dbffb8-aaa8-467a-a40d-03d4f6aef686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    if not is_safe_query(query):\n",
    "        return \"Query blocked due to unsafe content.\"\n",
    "\n",
    "    relevant_sections = select_relevant_sections(query, chunks)\n",
    "    if not relevant_sections:\n",
    "        return \"Not Found\"\n",
    "\n",
    "    prompt = build_prompt(query, relevant_sections)\n",
    "    response = llm.invoke(prompt).content\n",
    "    safe_response = filter_llm_response(response, relevant_sections)\n",
    "    return safe_response + \"\\n\\n(Feedback: Was this helpful? [Yes/No])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb96b3-0ef7-4756-92c0-dfda2a58b48b",
   "metadata": {},
   "source": [
    "### Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c1d83a-60dd-43d9-a678-3d9d45b57dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The case of Acute Myocardial Infarction in a Patient with Longstanding Type 2 Diabetes Mellitus involves a 54-year-old male with a 15-year history of type 2 diabetes and hypertension. He presented with acute-onset central chest pain radiating to the left arm, accompanied by diaphoresis, which began 2 hours prior to his arrival at the hospital. \n",
      "\n",
      "Upon examination, his blood pressure was recorded at 146/90 mmHg, and his heart rate was 102 beats per minute. An ECG showed ST-segment elevation in leads V2–V6, and his Troponin-I levels were elevated at 14 ng/mL (reference <0.04). His HbA1c was 9.1%, indicating poor glycemic control.\n",
      "\n",
      "The diagnosis was confirmed as an acute ST-elevation myocardial infarction (STEMI) affecting the anterior wall. The management included administering a loading dose of aspirin (325 mg) and ticagrelor (180 mg), along with an intravenous heparin infusion. The report emphasizes the importance of early recognition and the use of dual antiplatelet therapy in the management of such cases.\n",
      "\n",
      "(Feedback: Was this helpful? [Yes/No])\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe the case of Acute Myocardial Infarction in a Patient with Longstanding Type 2 Diabetes Mellitus\"\n",
    "print(answer_query(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6c531-16dc-4358-8b64-4cbedbc0dc48",
   "metadata": {},
   "source": [
    "### Medical Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ce8e90c-e1a8-45cd-a41d-cf50b3359298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chat(message, history):\n",
    "    # Use your new pipeline instead of conversation_chain\n",
    "    response = answer_query(message)  # This runs keyword match + guardrails + red teaming\n",
    "    return response\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420aa449-38ee-44ac-a3b6-5f793734cba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d10f56-f03f-4983-b09a-833a9b866d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
